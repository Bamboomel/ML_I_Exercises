{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i0ChBBhxF0K"
   },
   "source": [
    "# Linear Classification\n",
    "\n",
    "In this lab you will implement parts of a linear classification model using the regularized empirical risk minimization principle. By completing this lab and analysing the code, you gain deeper understanding of these type of models, and of gradient descent.\n",
    "\n",
    "\n",
    "## Problem Setting\n",
    "\n",
    "The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each of the patients is classified into two categories: normal (1) and abnormal (0). The training data contains 80 SPECT images from which 22 binary features have been extracted. The goal is to predict the label for an unseen test set of 187 tomography images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qSGEB3UkxF0L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "testfile = urllib.request.URLopener()\n",
    "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\", \"SPECT.train\")\n",
    "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.test\", \"SPECT.test\")\n",
    "\n",
    "df_train = pd.read_csv('SPECT.train',header=None)\n",
    "df_test = pd.read_csv('SPECT.test',header=None)\n",
    "\n",
    "train = df_train.values\n",
    "test = df_test.values\n",
    "\n",
    "y_train = train[:,0]\n",
    "X_train = train[:,1:]\n",
    "y_test = test[:,0]\n",
    "X_test = test[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBPhAtmexF0N"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Analyze the function learn_reg_ERM(X,y,lambda) which for a given $n\\times m$ data matrix $\\textbf{X}$ and binary class label $\\textbf{y}$ learns and returns a linear model $\\textbf{w}$.\n",
    "The binary class label has to be transformed so that its range is $\\left \\{-1,1 \\right \\}$. \n",
    "The trade-off parameter between the empirical loss and the regularizer is given by $\\lambda > 0$. \n",
    "To adapt the learning rate the Barzilai-Borwein method is used.\n",
    "\n",
    "Try to understand each step of the learning algorithm and comment each line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1-HgTxIQxF0N"
   },
   "outputs": [],
   "source": [
    "def learn_reg_ERM(X,y,lbda):\n",
    "    #specify how many iterations algorithm/loop should run\n",
    "    max_iter = 200\n",
    "    \n",
    "    #limit: if the weight is lower than the value e the loop will be stopped\n",
    "    e  = 0.001\n",
    "    \n",
    "    #learning rate\n",
    "    alpha = 1.\n",
    "\n",
    "    # initialising weight vector with random values     \n",
    "    w = np.random.randn(X.shape[1]);\n",
    "    \n",
    "    \n",
    "    for k in np.arange(max_iter):\n",
    "        # dot product of the input data and the weight\n",
    "        h = np.dot(X,w)\n",
    "        \n",
    "        # calculating the loss by comparing the calculated dot product and real value y \n",
    "        l,lg = loss(h, y)\n",
    "        \n",
    "        # printing the current loss in a clear format by calulcating it's mean across all values\n",
    "        print ('loss: {}'.format(np.mean(l)))\n",
    "        \n",
    "        # the function reg should ultimately compute the regulisation term (r) \n",
    "        # as well as it's gradient (rg - de: Ableitung) \n",
    "        r,rg = reg(w, lbda)\n",
    "        \n",
    "        # calculate the gradient of the loss function itself\n",
    "        # this gradient will then point us in the direction of the loss functions steepest descent (Minima) ?\n",
    "        g = np.dot(X.T,lg) + rg \n",
    "        \n",
    "        # we want the loss function to be minimal\n",
    "        # calculated by adding the dot product of the transposed input vector (X) \n",
    "        # and the gradient of the loss term with the gradient of the regulisation term\n",
    "        if (k > 0):\n",
    "            \n",
    "            # calulate the new step size for the following caluclation of w\n",
    "            # terms and values: \n",
    "                # alpha = learning rate - step size for updating our model\n",
    "                # g_old.T = previous gradients transposed\n",
    "                # g_old = previous gradients\n",
    "                # g = gradient of the loss function\n",
    "            # we change the alpha based on how much the gradient chaged \n",
    "            alpha = alpha * (np.dot(g_old.T,g_old))/(np.dot((g_old - g).T,g_old)) \n",
    "            # updated to make model more stable \n",
    "        \n",
    "        # update the weight vector using our new gradient and newly calulated alpha\n",
    "        # points in direction of steepest descent due to '-'\n",
    "        w = w - alpha * g\n",
    "        \n",
    "        # check the change in weight and compare it to limit e\n",
    "        # check if the iteration should be stopped (happens if improvement very small)\n",
    "        if (np.linalg.norm(alpha * g) < e):\n",
    "            break\n",
    "            \n",
    "        # save current gradient from this iteration to work with it in the next iteration\n",
    "        g_old = g\n",
    "    \n",
    "    # return weight vector\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1CmQjDhxF0O"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Fill in the code for the function loss(h,y) which computes the hinge loss and its gradient. \n",
    "This function takes a given vector $\\textbf{y}$ with the true labels $\\in \\left \\{-1,1\\right \\}$ and a vector $\\textbf{h}$ with the function values of the linear model as inputs. The function returns a vector $\\textbf{l}$ with the hinge loss $\\max(0, 1 − y_{i} h_{i})$ and a vector $\\textbf{g}$ with the gradients of the hinge loss w.r.t $\\textbf{h}$. (Note: The partial derivative of the hinge loss with respect to $\\textbf{h}$  is $g_{i} = −y $ if $l_{i} > 0$, else $g_{i} = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vct3IsAYxF0O"
   },
   "outputs": [],
   "source": [
    "def loss(h, y):\n",
    "    l = []\n",
    "    g = []\n",
    "    for hi, yi in zip(h,y):\n",
    "        l_current = max(0, 1-hi*yi) #hi = fTeta(xi)\n",
    "        l.append (l_current)\n",
    "        if l_current > 0:\n",
    "            g.append(-yi)\n",
    "        else:\n",
    "            g.append(e)\n",
    "    return l, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(h, y) :\n",
    "    l: np.array = np.maximum(0, 1 - h*y)\n",
    "    g: np.array = np.zeros(h. shape)\n",
    "    g[np.where(1 > 0)] = y[np.where(1 > 0)] * -1\n",
    "    return l, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(h, y):\n",
    "    # compute hinge loss \n",
    "    l = np.maximum(0, 1 - h * y)\n",
    "    \n",
    "    # compute gradient of hinge loss\n",
    "    g = np.where(1 > 0, -y, 0)\n",
    "    \n",
    "    return l, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmdwcNAaxF0P"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Fill in the code for the function reg(w,lambda) which computes the $\\mathcal{L}_2$-regularizer and the gradient of the regularizer function at point $\\textbf{w}$. \n",
    "\n",
    "\n",
    "$$r = \\frac{\\lambda}{2} \\textbf{w}^{T}\\textbf{w}$$\n",
    "\n",
    "$$g = \\lambda \\textbf{w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CEsrqBTfxF0P"
   },
   "outputs": [],
   "source": [
    "def reg(w, lbda):\n",
    "    # compute l2-regulariser \n",
    "    r: np.array = (lbda/2) * np.dot(w.T, w)\n",
    "        \n",
    "    # compute gradient of l2-regulariser\n",
    "    g: np.array = (lbda * w)\n",
    "    return r, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXlyhFPmxF0Q"
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "Fill in the code for the function predict(w,x) which predicts the class label $y$ for a data point $\\textbf{x}$ or a matrix $X$ of data points (row-wise) for a previously trained linear model $\\textbf{w}$. If there is only a data point given, the function is supposed to return a scalar value. If a matrix is given a vector of predictions is supposed to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bnKXor1NxF0Q"
   },
   "outputs": [],
   "source": [
    "def predict(w, X):\n",
    "    outputs = np.dot(X,w)\n",
    "    \n",
    "    preds = outputs\n",
    "    for value_idx in range(len(outputs)):\n",
    "        if outputs[value_idx] >= 0:\n",
    "            preds[value_idx] = 1\n",
    "        else:\n",
    "            preds[value_idx] = -1\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possibility using: 2*h-1 ? - gives 1 and -1\n",
    "# or np.sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltgVMtXIxF0R"
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "#### 5.1 \n",
    "Train a linear model on the training data and classify all 187 test instances afterwards using the function predict. \n",
    "Please note that the given class labels are in the range $\\left \\{0,1 \\right \\}$, however the learning algorithm expects a label in the range of $\\left \\{-1,1 \\right \\}$. Then, compute the accuracy of your trained linear model on both the training and the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LqdCXWWYxF0R",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4607794994186016\n",
      "loss: 10.991022914453243\n",
      "loss: 1058.0249999999983\n",
      "Training Accuracy: 50.0%\n",
      "Test Accuracy: 91.98%\n"
     ]
    }
   ],
   "source": [
    "# Transform the class labels to be in the range of {-1, 1) instead of {0, 1}.\n",
    "y_train_t = y_train * 2 - 1\n",
    "y_test_t = y_test * 2 - 1\n",
    "\n",
    "\n",
    "# Set the regularization parameter lambda\n",
    "lbda = 0.01\n",
    "\n",
    "\n",
    "# Train the linear model on the training data\n",
    "w = learn_reg_ERM(X_train, y_train_t, lbda)\n",
    "\n",
    "# Make predictions on the training and test data\n",
    "y_train_pred = predict(w, X_train)\n",
    "y_test_pred = predict(w, X_test)\n",
    "\n",
    "# Calculate the accuracy on the training and test data\n",
    "train_accuracy = np.mean(y_train_pred == y_train_t)\n",
    "test_accuracy = np.mean (y_test_pred == y_test_t)\n",
    "\n",
    "print(f'Training Accuracy: {round(train_accuracy*100, 2)}%')\n",
    "print(f'Test Accuracy: {round(test_accuracy*100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss only printed 3 times ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFdpQNg3xF0S"
   },
   "source": [
    "#### 5.2\n",
    "Compare the accuracy of the linear model with the accuracy of a random forest and a decision tree (non-linear models) on the training and test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "l_u_jEmXxF0T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy: 93.75%\n",
      "Random Forest Test Accuracy: 78.07%\n",
      "Dicision Tree Training Accuracy: 93.75%\n",
      "Dicision Tree Test Accuracy: 67.91%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize classifiers\n",
    "rf = RandomForestClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifiers\n",
    "rf.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training data\n",
    "y_train_pred_rf = rf.predict(X_train)\n",
    "y_train_pred_dt = dt.predict(X_train)\n",
    "\n",
    "# Predict the labels of the test data \n",
    "y_test_pred_rf = rf.predict(X_test)\n",
    "y_test_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Compute the training accuracy \n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "train_accuracy_dt = accuracy_score(y_train, y_train_pred_dt)\n",
    "\n",
    "# Compute the test accuracy \n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "test_accuracy_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "\n",
    "# Print accuracies \n",
    "print(f'Random Forest Training Accuracy: {round(train_accuracy_rf*100, 2)}%')\n",
    "print(f'Random Forest Test Accuracy: {round(test_accuracy_rf*100, 2)}%')\n",
    "\n",
    "print(f'Dicision Tree Training Accuracy: {round(train_accuracy_dt*100, 2)}%')\n",
    "print(f'Dicision Tree Test Accuracy: {round(test_accuracy_dt*100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab06_LinearClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
